{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computación Avanzada y sus Aplicaciones a Ingeniería\n",
    "\n",
    "### Máster Universitario en Ingeniería Informática"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 2 - Parte II - Introducción a SparkSQL\n",
    "\n",
    "En esta práctica introducimos las operaciones básicas para trabajar con los DataFrames de Spark. Este primer notebook no es más que una guía de todas las operaciones que puedes realizar en SparkSQL con DataFrames. Sigue detenidamente todos los bloques y prueba a cambiar los valores establecidos para comprobar su funcionamiento.\n",
    "\n",
    "Ten en cuenta que una vez tengas en marcha Spark, podrás visualizar la evolución de cada trabajo de Spark en  <http://localhost:4040>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Uso básico de los notebooks y su integración con Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilización de un Notebook\n",
    "\n",
    "Un notebook está compuesto por una serie de celdas. Estas celdas pueden contener texto explicativo o código, pero nunca se mezclan ambas en la misma celda. Cuando ejecutamos una celda de texto, lo que hemos escrito con el lenguaje de markdown se renderiza como texto, imágenes y links (como si fuera HTML). El texto que estás leyendo ahora mismo es parte de una celda de este tipo. Las celdas con código Python te permiten ejecutar comandos de Python como si estuvieras en la consola de Python. Coloca el cursos dentro de la celda de más abajo y presiona \"Shift + Enter\" para ejecutar el código y avanzar a la siguiente celda. También puedes utilizar \"Ctrl + Enter\" para ejecutar el código y mantenerte en la misma celda. Estos comandos funcionan tanto en celdas de código como en celdas de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esto es una celda ed Python. Puedes ejecutar código Python en estas celdas\n",
    "print('La suma de 1 y 1 es {0}'.format(1+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta es otra celda Python, utiliza una variable x y un if\n",
    "x = 28\n",
    "if x > 18:\n",
    "    print('x es mayor que 18')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estado de un Notebook\n",
    "\n",
    "Cuando trabajas con un notebook es importante ejecutar todas las celdas con código. El notebook tiene estado, lo que quiere decir que las variables y sus valores se mantienen hasta el que el kernel del notebook se reinicia. Si no ejecutas todas las celdas de código a lo largo del notebook, las variables pueden no estar correctamente inicializadas y pueden fallar celdas de código posteriores. También necesitarás reejecutar cualquier celda que hayas modificado para que los cambios estén disponibles en otras celdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta celda utiliza la variable x que hemos definido en una celda anterior\n",
    "# Si no ejecutamos la celda anaterior este código fallará\n",
    "print(x * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a SparkSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En caso de estar utilizando pySpark, **NO** es necesario inicializar el `SparkSession`, es decir, **no** ejecutar la siguiente celda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Ejemplo pySparkSQL\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API de SparkSQL\n",
    "<http://spark.apache.org/docs/latest/api/python/pyspark.sql.html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de un DataFrame\n",
    "Podemos crear un DataFrame de diferente formas\n",
    "\n",
    "1. Infiriendo el esquema automáticamente a partir de los datos\n",
    "1. Infiriendo el esquema automáticamente a partir de los metadatos\n",
    "1. Definiendo explícitamente el esquema\n",
    "\n",
    "A su vez, al igual que con los RDDs podemos crearlo a partir de dos fuentes diferentes de datos\n",
    "\n",
    "1. Cargando un conjunto de datos almacenado en un medio externo\n",
    "2. Distribuyendo una colección de objetos existente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencia del esquema a partir de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuplas = [('Alice', 1), ('Bob', 4), ('Juan', 10), ('Pepe', 25), ('Panchito', 15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Alice', 1), ('Bob', 4), ('Juan', 10), ('Pepe', 25), ('Panchito', 15)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuplas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(tuplas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: string, _2: bigint]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|   _1| _2|\n",
      "+-----+---+\n",
      "|Alice|  1|\n",
      "|  Bob|  4|\n",
      "| Juan| 10|\n",
      "| Pepe| 25|\n",
      "+-----+---+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1='Alice', _2=1),\n",
       " Row(_1='Bob', _2=4),\n",
       " Row(_1='Juan', _2=10),\n",
       " Row(_1='Pepe', _2=25),\n",
       " Row(_1='Panchito', _2=15)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(tuplas, ['name', 'age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    name|age|\n",
      "+--------+---+\n",
      "|   Alice|  1|\n",
      "|     Bob|  4|\n",
      "|    Juan| 10|\n",
      "|    Pepe| 25|\n",
      "|Panchito| 15|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [(1, ), (2, ), (3,), (4,)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: bigint]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sin nombre para las columnas: [Row(_1='Alice', _2=1), Row(_1='Bob', _2=4), Row(_1='Juan', _2=10), Row(_1='Pepe', _2=25), Row(_1='Panchito', _2=15)]\n",
      "Con nombre para las columnas: [Row(name='Alice', age=1), Row(name='Bob', age=4), Row(name='Juan', age=10), Row(name='Pepe', age=25), Row(name='Panchito', age=15)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sin nombre para las columnas: \" + str(spark.createDataFrame(tuplas).collect()))\n",
    "print(\"Con nombre para las columnas: \" + str(spark.createDataFrame(tuplas, ['name', 'age']).collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(tuplas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, age: bigint]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.toDF(['name', 'age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, age: bigint]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(rdd, ['name', 'age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sin nombre para las columnas: \" + str(spark.createDataFrame(rdd).collect()))\n",
    "print(\"Con nombre para las columnas: \" + str(spark.createDataFrame(rdd, ['name', 'age']).collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Row(name='mikel', age=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mikel'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mikel'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Row is read-only",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8832\\301001076.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'pepe'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\py38ml\\lib\\site-packages\\pyspark\\sql\\types.py\u001b[0m in \u001b[0;36m__setattr__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   1904\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1905\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"__fields__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1906\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Row is read-only\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1907\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Row is read-only"
     ]
    }
   ],
   "source": [
    "p.name = 'pepe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Person = Row('name', 'age') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(name='mikel', age=36)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Person('mikel', 36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = rdd.map(lambda t: Person(*t)).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "Person = Row('name', 'age') # Creamos una Row con los índices para poder crear filas con datos utilizándola\n",
    "person = rdd.map(lambda r: Person(*r)) # con *r lo que hacemos es pasar la tupla como parámetro a la fila y crearla directamente con dichos datos\n",
    "df2 = spark.createDataFrame(person)\n",
    "df2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Juan</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pepe</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Panchito</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name  age\n",
       "0     Alice    1\n",
       "1       Bob    4\n",
       "2      Juan   10\n",
       "3      Pepe   25\n",
       "4  Panchito   15"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandaDF = df2.toPandas()\n",
    "print(\"DataFrame de Panda obtenido de un DataFrame Spark:\")\n",
    "print(pandaDF)\n",
    "print(\"DataFrame de Spark creado a partir del Panda:\")\n",
    "print(spark.createDataFrame(pandaDF).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Especificación del esquema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', age=1),\n",
       " Row(name='Bob', age=4),\n",
       " Row(name='Juan', age=10),\n",
       " Row(name='Pepe', age=25),\n",
       " Row(name='Panchito', age=15)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)])\n",
    "\n",
    "\n",
    "\n",
    "df3 = spark.createDataFrame(rdd, schema)\n",
    "df3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(a='Alice', b=1), Row(a='Bob', b=4), Row(a='Juan', b=10), Row(a='Pepe', b=25), Row(a='Panchito', b=15)]\n",
      "[Row(value=1), Row(value=4), Row(value=10), Row(value=25), Row(value=15)]\n"
     ]
    }
   ],
   "source": [
    "print(spark.createDataFrame(rdd, \"a: string, b: int\").collect())\n",
    "\n",
    "rdd2 = rdd.map(lambda row: row[1])\n",
    "print(spark.createDataFrame(rdd2, \"int\").collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "data = [\n",
    "    Row(\n",
    "        name=\"Charlie\",\n",
    "        num_pets=1,\n",
    "        paid_in_full=True,\n",
    "        preferences={},\n",
    "        registered_on=datetime.datetime(2016, 5, 1, 12,0)\n",
    "    ),\n",
    "\n",
    "    Row(\n",
    "        name=\"Alex\",\n",
    "        num_pets=3,\n",
    "        paid_in_full=True,\n",
    "        preferences={\n",
    "            \"preferred_vet\": \"Dr. Smith\",\n",
    "            \"preferred_appointment_day\": \"Monday\"\n",
    "        },\n",
    "        registered_on=datetime.datetime(2015, 1, 1, 12,0),\n",
    "        visits=[\n",
    "            datetime.datetime(2015, 2, 1, 11, 0),\n",
    "            datetime.datetime(2015, 2, 2, 10, 45),\n",
    "        ],\n",
    "    ), \n",
    "\n",
    "\n",
    "    Row(\n",
    "        name=\"Beth\",\n",
    "        num_pets=2,\n",
    "        paid_in_full=False,\n",
    "        preferences={\n",
    "            \"preferred_vet\": \"Dr. Travis\",\n",
    "        },\n",
    "        registered_on=datetime.datetime(2013, 1, 1, 12,0),\n",
    "        visits=[\n",
    "            datetime.datetime(2015, 1, 15, 12, 15),\n",
    "            datetime.datetime(2015, 2, 1, 11, 15),\n",
    "        ],\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Charlie', num_pets=1, paid_in_full=True, preferences={}, registered_on=datetime.datetime(2016, 5, 1, 12, 0)),\n",
       " Row(name='Alex', num_pets=3, paid_in_full=True, preferences={'preferred_vet': 'Dr. Smith', 'preferred_appointment_day': 'Monday'}, registered_on=datetime.datetime(2015, 1, 1, 12, 0), visits=[datetime.datetime(2015, 2, 1, 11, 0), datetime.datetime(2015, 2, 2, 10, 45)]),\n",
       " Row(name='Beth', num_pets=2, paid_in_full=False, preferences={'preferred_vet': 'Dr. Travis'}, registered_on=datetime.datetime(2013, 1, 1, 12, 0), visits=[datetime.datetime(2015, 1, 15, 12, 15), datetime.datetime(2015, 2, 1, 11, 15)])]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencia del esquema a partir de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD: Esquema inferido a partir de la primera fila.\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- num_pets: long (nullable = true)\n",
      " |-- paid_in_full: boolean (nullable = true)\n",
      " |-- preferences: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- registered_on: timestamp (nullable = true)\n",
      " |-- visits: array (nullable = true)\n",
      " |    |-- element: timestamp (containsNull = true)\n",
      "\n",
      "RDD: Esquema inferido de un sampling aleatorio.\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- num_pets: long (nullable = true)\n",
      " |-- paid_in_full: boolean (nullable = true)\n",
      " |-- preferences: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- registered_on: timestamp (nullable = true)\n",
      " |-- visits: array (nullable = true)\n",
      " |    |-- element: timestamp (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creamos un RDD con los datos de ejemplo\n",
    "dataRDD = sc.parallelize(data)\n",
    "\n",
    "# Creamos un DataFrame a partir del RDD, \n",
    "# infiriendo el esquema a partir de la primera Row\n",
    "print(\"RDD: Esquema inferido a partir de la primera fila.\")\n",
    "# Por defecto samplingRatio=None\n",
    "dataDF = spark.createDataFrame(dataRDD)\n",
    "dataDF.printSchema()\n",
    "\n",
    "# Creamos un DataFrame a partir del RDD,\n",
    "# infiriendo el esquema a partir de un sampling de las Rows\n",
    "print(\"RDD: Esquema inferido de un sampling aleatorio.\")\n",
    "dataDF = spark.createDataFrame(dataRDD, samplingRatio=0.6)\n",
    "dataDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Especificación del esquema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD: Esquema espcificado explícitamente.\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- num_pets: integer (nullable = true)\n",
      " |-- paid_in_full: boolean (nullable = true)\n",
      " |-- preferences: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- registered_on: date (nullable = true)\n",
      " |-- visits: array (nullable = true)\n",
      " |    |-- element: timestamp (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import ArrayType, BooleanType, DateType, \\\n",
    "    IntegerType, MapType, StringType, TimestampType, StructField, StructType\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"num_pets\", IntegerType(), True),\n",
    "        StructField(\"paid_in_full\", BooleanType(), True),\n",
    "        StructField(\"preferences\", MapType(StringType(), StringType(), True), True),\n",
    "        StructField(\"registered_on\", DateType(), True),\n",
    "        StructField(\"visits\", ArrayType(TimestampType(), True), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Crear un DataFrame a partir de un RDD, \n",
    "# especificando el esquema\n",
    "print(\"RDD: Esquema espcificado explícitamente.\")\n",
    "dataDF = spark.createDataFrame(data, schema)\n",
    "dataDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura del DataFrame desde un fichero JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON: Esquema inferido a partir de todas las filas.\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- num_pets: long (nullable = true)\n",
      " |-- paid_in_full: boolean (nullable = true)\n",
      " |-- registered_on: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crear un DataFrame a partir de un fichero JSON,\n",
    "# infiriendo el esquema a partir de todas las filas\n",
    "print(\"JSON: Esquema inferido a partir de todas las filas.\")\n",
    "dataDF = spark.read.option(\"samplingRatio\", 0.333).json(\"data/data.json\")\n",
    "dataDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON: Esquema especificado explícitamente.\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- num_pets: integer (nullable = true)\n",
      " |-- paid_in_full: boolean (nullable = true)\n",
      " |-- preferences: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- registered_on: date (nullable = true)\n",
      " |-- visits: array (nullable = true)\n",
      " |    |-- element: timestamp (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crear un DataFrame a partir de un fichero JSON,\n",
    "# especificando el esquema\n",
    "print(\"JSON: Esquema especificado explícitamente.\")\n",
    "dataDF = spark.read.json(\"data/data.json\", schema)\n",
    "dataDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acceso a las columnas de un DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2.age)\n",
    "print(df2[\"age\"])\n",
    "print((df2.age + 1).alias(\"cumpleaños\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    name|age|\n",
      "+--------+---+\n",
      "|   Alice|  1|\n",
      "|     Bob|  4|\n",
      "|    Juan| 10|\n",
      "|    Pepe| 25|\n",
      "|Panchito| 15|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'age'>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'age'>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"age\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sql_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'age'>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_f.col('age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operaciones con DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos realizar dos tipos de operaciones sobre los DataFrames al igual que con los RDDs.\n",
    "1. Transformaciones: Crean un nuevo DataFrame a partir de otro - **EVALUACIÓN VAGA (LAZY)** - hasta que no se ejecuta una acción no se realiza la transformación\n",
    "2. Acciones: Utilizan el DataFrame para lograr un resultado que es recibido por el driver (o escriben el DataFrame a disco)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformaciones sobre DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaciones básicas\n",
    "\n",
    "\n",
    "Transformación | Descripción\n",
    "------------- | -------------\n",
    "*select(*\\**cols)* | Devuelve un nuevo DataFrame proyectando una serie de expresiones que pueden ser nombres de columnas o expresiones de tipo Column. Si se utiliza \"\\*\", todas las columnas del DataFrame se proyectan al nuevo DataFrame\n",
    "*selectExpr(*\\**expr)* | Variante de select que admite expresiones SQL\n",
    "*filter(condition) / where(condition)* | Filtra las filas usando la condición especificada\n",
    "*orderBy(*\\**cols, ascending)* | Devuelve un nuevo DataFrame ordenado por las columnas especificadas. Por defecto en orden ascendente\n",
    "*sort(*\\**cols, *\\*\\**kwargs)* | Devuelve un DataFrame ordenado por las columnas especificadas. La condición debe ser tipo Column o expresión SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    name|age|\n",
      "+--------+---+\n",
      "|   Alice|  1|\n",
      "|     Bob|  4|\n",
      "|    Juan| 10|\n",
      "|    Pepe| 25|\n",
      "|Panchito| 15|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame de ejemplo\n",
    "tuplas = [('Alice', 1), ('Bob', 4), ('Juan', 10), ('Pepe', 25), ('Panchito', 15)]\n",
    "df = spark.createDataFrame(tuplas, ['name', 'age'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    name|\n",
      "+--------+\n",
      "|   Alice|\n",
      "|     Bob|\n",
      "|    Juan|\n",
      "|    Pepe|\n",
      "|Panchito|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'(age * 2) AS age2'>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df[\"age\"] * 2).alias('age2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+\n",
      "|    name|age2|\n",
      "+--------+----+\n",
      "|   Alice|   2|\n",
      "|     Bob|   8|\n",
      "|    Juan|  20|\n",
      "|    Pepe|  50|\n",
      "|Panchito|  30|\n",
      "+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df[\"name\"], (df[\"age\"] * 2).alias('age2')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `select(*cols)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.select('*').collect())\n",
    "\n",
    "print(df.select('name', 'age').collect())\n",
    "\n",
    "print(df.select(df.name, (df.age + 10).alias('age')).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `selectExpr(*expr)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Column '`age * 2 as age2`' does not exist. Did you mean one of the following? [age, name];\n'Project ['age * 2 as age2]\n+- LogicalRDD [name#349, age#350L], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8832\\4256496156.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'age * 2 as age2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\py38ml\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   2021\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Alice'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Bob'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2022\u001b[0m         \"\"\"\n\u001b[1;32m-> 2023\u001b[1;33m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2024\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py38ml\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py38ml\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Column '`age * 2 as age2`' does not exist. Did you mean one of the following? [age, name];\n'Project ['age * 2 as age2]\n+- LogicalRDD [name#349, age#350L], false\n"
     ]
    }
   ],
   "source": [
    "df.select('age * 2 as age2').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                age2|\n",
      "+--------------------+\n",
      "|    7.38905609893065|\n",
      "|  2980.9579870417283|\n",
      "| 4.851651954097903E8|\n",
      "|5.184705528587072E21|\n",
      "|1.068647458152446...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr('exp(age * 2) as age2').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.selectExpr(\"age * 2\", \"abs(age)\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `filter(condition) = where(condition)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    name|age|\n",
      "+--------+---+\n",
      "|    Juan| 10|\n",
      "|    Pepe| 25|\n",
      "|Panchito| 15|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"age > 8\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    name|age|\n",
      "+--------+---+\n",
      "|    Juan| 10|\n",
      "|    Pepe| 25|\n",
      "|Panchito| 15|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df[\"age\"] > 8).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modo programación orientada a objetos (POO)\n",
    "print(\"Modo POO, edad > 18: \" + str( df.filter(df.age > 18).collect() ))\n",
    "\n",
    "print(\"Modo POO, edad == 1: \" + str( df.where(df.age == 1).collect() ))\n",
    "\n",
    "# Modo SQL\n",
    "print(\"Modo SQL, edad > 18: \" + str( df.filter(\"age > 18\").collect() ))\n",
    "\n",
    "print(\"Modo SQL, edad = 1: \" + str( df.where(\"age = 1\").collect() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `orderBy(*cols, ascending)` y `sort(*cols, **kwargs)`\n",
    "\n",
    "Tienen diferentes formas de recibir los argumentos, pero el resultado es el mismo en ambas. En ambas se reciben primero las columnas por las que ordenar el DataFrame y después la forma en la que se quiere ordenar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    name|age|\n",
      "+--------+---+\n",
      "|    Pepe| 25|\n",
      "|Panchito| 15|\n",
      "|    Juan| 10|\n",
      "|     Bob|  4|\n",
      "|   Alice|  1|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(\"age\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    name|age|\n",
      "+--------+---+\n",
      "|    Pepe| 25|\n",
      "|Panchito| 15|\n",
      "|    Juan| 10|\n",
      "|     Bob|  4|\n",
      "|   Alice|  1|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(df.age.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordenar por edad descendente:\n",
      "[Row(name='Pepe', age=25), Row(name='Panchito', age=15), Row(name='Juan', age=10), Row(name='Bob', age=4), Row(name='Alice', age=1)]\n",
      "[Row(name='Pepe', age=25), Row(name='Panchito', age=15), Row(name='Juan', age=10), Row(name='Bob', age=4), Row(name='Alice', age=1)]\n",
      "[Row(name='Pepe', age=25), Row(name='Panchito', age=15), Row(name='Juan', age=10), Row(name='Bob', age=4), Row(name='Alice', age=1)]\n",
      "Ordenar por edad ascendente:\n",
      "[Row(name='Alice', age=1), Row(name='Bob', age=4), Row(name='Juan', age=10), Row(name='Panchito', age=15), Row(name='Pepe', age=25)]\n",
      "Ordenar por edad descendente y nombre ascendente:\n",
      "[Row(name='Pepe', age=25), Row(name='Panchito', age=15), Row(name='Juan', age=10), Row(name='Bob', age=4), Row(name='Alice', age=1)]\n",
      "[Row(name='Pepe', age=25), Row(name='Panchito', age=15), Row(name='Juan', age=10), Row(name='Bob', age=4), Row(name='Alice', age=1)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Ordenar por edad descendente:\")\n",
    "print(df.sort(df.age.desc()).collect())\n",
    "print(df.sort(\"age\", ascending=False).collect())\n",
    "print(df.orderBy(df.age.desc()).collect())\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "print(\"Ordenar por edad ascendente:\")\n",
    "print(df.sort(asc(\"age\")).collect())\n",
    "\n",
    "print(\"Ordenar por edad descendente y nombre ascendente:\")\n",
    "print(df.orderBy(desc(\"age\"), \"name\").collect())\n",
    "print(df.orderBy([\"age\", \"name\"], ascending=[0, 1]).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaciones adicionales\n",
    "\n",
    "\n",
    "Transformación | Descripción\n",
    "------------- | -------------\n",
    "*distinct()* | Devuelve un nuevo DataFrame con las filas únicas del original\n",
    "*dropDuplicates(*\\**cols)* | Devuelve un nuevo DataFrame sin filas duplicadas considerando las columnas especificadas\n",
    "*withColumn(colName, col)* | Devuelve un nuevo DataFrame añadiendo una nueva columna o reemplazando la columna existente con el mismo nombre\n",
    "*withColumnRenamed(existing, new)* | Devuelve un DataFrame renombrando una columna existente\n",
    "*drop(col)* | Devuelve un nuevo DataFrame con la columna especificada eliminada\n",
    "*limit(num)* | Limita el número de filas obtenidas como resultado\n",
    "*cache()* | Mantiene el DataFrame almacenado en memoria para ser reusado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `distinct()` y `dropDuplicates(*cols)`\n",
    "La diferencia entre ambos está en que en `dropDuplicates` podemos especificar los campos por los cuales decidimos que dos filas están repetidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame:\n",
      "+-----+---+------+\n",
      "| name|age|height|\n",
      "+-----+---+------+\n",
      "|Alice|  5|    80|\n",
      "|Alice|  5|    80|\n",
      "|Alice| 10|    80|\n",
      "+-----+---+------+\n",
      "\n",
      "distinct(): \n",
      "+-----+---+------+\n",
      "| name|age|height|\n",
      "+-----+---+------+\n",
      "|Alice|  5|    80|\n",
      "|Alice| 10|    80|\n",
      "+-----+---+------+\n",
      "\n",
      "dropDuplicates(): \n",
      "+-----+---+------+\n",
      "| name|age|height|\n",
      "+-----+---+------+\n",
      "|Alice|  5|    80|\n",
      "|Alice| 10|    80|\n",
      "+-----+---+------+\n",
      "\n",
      "dropDuplicates('name', 'height')\n",
      "+-----+---+------+\n",
      "| name|age|height|\n",
      "+-----+---+------+\n",
      "|Alice|  5|    80|\n",
      "+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "df2 = sc.parallelize([ \\\n",
    "    Row(name='Alice', age=5, height=80), \\\n",
    "    Row(name='Alice', age=5, height=80), \\\n",
    "    Row(name='Alice', age=10, height=80)]).toDF()\n",
    "print(\"DataFrame:\")\n",
    "df2.show()\n",
    "\n",
    "print(\"distinct(): \")\n",
    "df2.distinct().show()\n",
    "\n",
    "print(\"dropDuplicates(): \")\n",
    "df2.dropDuplicates().show() \n",
    "\n",
    "print(\"dropDuplicates('name', 'height')\")\n",
    "df2.dropDuplicates(['name', 'height']).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `drop(col) `\n",
    "Recordad que drop elimina la columna pero devuelve un nuevo DataFrame, no modifica el que tenemos (son inmutables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Eliminamos la columna age:\")\n",
    "print(df.drop('age').collect())\n",
    "print(df.drop(df.age).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop('age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `limit(num)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Limit(1):\" + str( df.limit(1).collect() ))\n",
    "print(\"Limit(0):\" + str( df.limit(0).collect() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `withColumn(colName, col) `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Añadimos una columna denominada age2 obtenida a partir de la columna age y sumando un 2:\n",
      "+--------+---+----+\n",
      "|    name|age|age2|\n",
      "+--------+---+----+\n",
      "|   Alice|  1|   3|\n",
      "|     Bob|  4|   6|\n",
      "|    Juan| 10|  12|\n",
      "|    Pepe| 25|  27|\n",
      "|Panchito| 15|  17|\n",
      "+--------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Añadimos una columna denominada age2 obtenida a partir de la columna age y sumando un 2:\")\n",
    "df.withColumn('age2', df.age + 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----+----+\n",
      "|    name|age|age2|age3|\n",
      "+--------+---+----+----+\n",
      "|   Alice|  1|   3|   4|\n",
      "|     Bob|  4|   6|   7|\n",
      "|    Juan| 10|  12|  13|\n",
      "|    Pepe| 25|  27|  28|\n",
      "|Panchito| 15|  17|  18|\n",
      "+--------+---+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('*', (df.age + 2).alias('age2'), (df.age + 3).alias('age3')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `withColumnRenamed(existing, new) `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Renombramos la columna age a age2 y obtenemos un nuevo DataFrame con el cambio:\")\n",
    "df.withColumnRenamed('age', 'age2').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaciones: Operaciones con columnas\n",
    "\n",
    "\n",
    "Operación | Descripción\n",
    "------------- | -------------\n",
    "*alias(*\\**alias)* | Devuelve la columna con un nuevo nombre\n",
    "*between(lowerBound, upperBound)* | True si el valor está entre los dos valores\n",
    "*isNull() / isNotNull()* | True si el valor es nulo y viceversa\n",
    "*when(condition, value) / otherwise(value)* | Evalúa una lista de condiciones y en base a estas devuelve un valor u otro\n",
    "*Startswith(other), substring(startPos, len), like(otheR)* | Funciones para operar con strings\n",
    "*isin(*\\**cols)* | True si el valor está en la lista de argumentos\n",
    "*explode(col)* | Devuelve una nueva fila para cada elemento en el array\n",
    "*lit(value)* | Crea una columna con el valor literal\n",
    "*length(col)* | Longitud de la columna\n",
    "\n",
    "Estas operaciones por si mismas solo devuelve una columna (expresión SQL). Para poder aplicalas al DataFrame se deben utilizar en conjunto con una transformación tipo `select`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `alias(*alias)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'age AS edad'>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['age'].alias('edad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|edad|\n",
      "+----+\n",
      "|   1|\n",
      "|   4|\n",
      "|  10|\n",
      "|  25|\n",
      "|  15|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df['age'].alias('edad')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columna cambio de nombre: \")\n",
    "print(df.name.alias(\"hola\"))\n",
    "\n",
    "print(\"\\nDataFrame con columna nombre renombrada a hola: \")\n",
    "df.select(df.name.alias(\"hola\")).show() # lógicamente solo aparece la columna seleccionada!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `between(lowerBound, upperBound) `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna con condición between: \n",
      "Column<'((age >= 18) AND (age <= 65))'>\n",
      "\n",
      "DataFrame con nombre y columna con True o False según se cumple la condición\n",
      "+--------+---------------------------+\n",
      "|    name|((age >= 2) AND (age <= 4))|\n",
      "+--------+---------------------------+\n",
      "|   Alice|                      false|\n",
      "|     Bob|                       true|\n",
      "|    Juan|                      false|\n",
      "|    Pepe|                      false|\n",
      "|Panchito|                      false|\n",
      "+--------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Columna con condición between: \")\n",
    "print(df.age.between(18, 65))\n",
    "print(\"\\nDataFrame con nombre y columna con True o False según se cumple la condición\")\n",
    "df.select(df.name, df.age.between(2, 4)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "| Bob|  4|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.age.between(2, 4)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `isNull() / isNotNull() `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Expresión Columna con isNull()/isNotNull():\")\n",
    "print(df.age.isNull())\n",
    "print(df.age.isNotNull())\n",
    "\n",
    "print(\"\\nDataFrame con filas que tienen el nombre nulo\")\n",
    "df.filter(df.name.isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `when(condition, value) / otherwise(value) `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "print(\"Condición when/otherwise:\")\n",
    "print(F.when(df.age > 18, \"adulto\").otherwise(\"peque\"))\n",
    "\n",
    "print(\"\\nEjemplos when/otherwise\")\n",
    "df.select(df.name, F.when(df.age > 3, 1).otherwise(0)).show()\n",
    "df.select(df.name, F.when(df.age > 4, 1).when(df.age < 3, -1).otherwise(0)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    name|age|\n",
      "+--------+---+\n",
      "|     Bob|  4|\n",
      "|    Juan| 10|\n",
      "|    Pepe| 25|\n",
      "|Panchito| 15|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as sql_f\n",
    "\n",
    "df.filter(sql_f.when(df.age > 3, True).otherwise(False)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Startswith(other), substring(startPos, len), like(otheR) `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columna like: \")\n",
    "print(df.name.like(\"M*\"))\n",
    "\n",
    "print(\"\\nDataFrame con primeras tres letras de los nombres\")\n",
    "df.select(df.name.substr(1, 3).alias(\"Inicial\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `isin(*cols) `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columna con expresión isin()\")\n",
    "print(df.name.isin(\"Mikel\", \"Pepe\"))\n",
    "\n",
    "\n",
    "# Ojo! Estamos usando la operación para filtrar las filas, para ello podemos usar df[condición]!\n",
    "print(\"\\nFiltrar filas con nombres Bob y Mikel\")\n",
    "print(df[df.name.isin(\"Bob\", \"Mikel\")].collect())\n",
    "print(\"Filtrar filas con edades 1, 2 o 3\")\n",
    "print(df[df.age.isin([1, 2, 3])].collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `explode(col) `\n",
    "Explode puede utilizarse para simular el flatMap de los RDDs, ya que crea tantas filas como valores tenga un array o map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n",
    "print(\"DataFrame original\")\n",
    "print(eDF.collect())\n",
    "\n",
    "print(\"\\nDataFrame con filas expandidas según el array de enteros\")\n",
    "eDF.select(F.explode(eDF.intlist).alias(\"anInt\")).show()\n",
    "print(\"DataFrame con filas expandidas según el map de caracteres\")\n",
    "eDF.select('*', F.explode(eDF.mapfield).alias(\"key\", \"value\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|   name|             hobbies|              avatar|\n",
      "+-------+--------------------+--------------------+\n",
      "|  Isaac|[Lifting, Running...|{hair_length -> s...|\n",
      "|  Mikel|[Triathlon, Runni...|{hair_length -> v...|\n",
      "|Daniele|  [Singing, Dancing]|{hair_length -> l...|\n",
      "|   Anne|    [Running, Music]|{hair_length -> m...|\n",
      "+-------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = spark.createDataFrame([\n",
    "    Row(name='Isaac', hobbies=[\"Lifting\", \"Running\", \"Reading\"], \n",
    "        avatar={\"hair\": \"black\",\"hair_length\": \"short\"}),\n",
    "    Row(name='Mikel', hobbies=[\"Triathlon\", \"Running\", \"Cycling\"], \n",
    "        avatar={\"hair\": \"blond\",\"hair_length\": \"very short\"}),\n",
    "    Row(name='Daniele', hobbies=[\"Singing\", \"Dancing\"], \n",
    "        avatar={\"hair\": \"brown\",\"hair_length\": \"large\"}),\n",
    "    Row(name='Anne', hobbies=[\"Running\", \"Music\"], \n",
    "        avatar={\"hair\": \"red\",\"hair_length\": \"middle\"})\n",
    "])\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|      col|\n",
      "+---------+\n",
      "|  Lifting|\n",
      "|  Running|\n",
      "|  Reading|\n",
      "|Triathlon|\n",
      "|  Running|\n",
      "|  Cycling|\n",
      "|  Singing|\n",
      "|  Dancing|\n",
      "|  Running|\n",
      "|    Music|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.select(sql_f.explode('hobbies')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+---------+-----------+----------+\n",
      "|   name|             hobbies|              avatar|   hobbie|      clave|     valor|\n",
      "+-------+--------------------+--------------------+---------+-----------+----------+\n",
      "|  Isaac|[Lifting, Running...|{hair_length -> s...|  Lifting|hair_length|     short|\n",
      "|  Isaac|[Lifting, Running...|{hair_length -> s...|  Lifting|       hair|     black|\n",
      "|  Isaac|[Lifting, Running...|{hair_length -> s...|  Running|hair_length|     short|\n",
      "|  Isaac|[Lifting, Running...|{hair_length -> s...|  Running|       hair|     black|\n",
      "|  Isaac|[Lifting, Running...|{hair_length -> s...|  Reading|hair_length|     short|\n",
      "|  Isaac|[Lifting, Running...|{hair_length -> s...|  Reading|       hair|     black|\n",
      "|  Mikel|[Triathlon, Runni...|{hair_length -> v...|Triathlon|hair_length|very short|\n",
      "|  Mikel|[Triathlon, Runni...|{hair_length -> v...|Triathlon|       hair|     blond|\n",
      "|  Mikel|[Triathlon, Runni...|{hair_length -> v...|  Running|hair_length|very short|\n",
      "|  Mikel|[Triathlon, Runni...|{hair_length -> v...|  Running|       hair|     blond|\n",
      "|  Mikel|[Triathlon, Runni...|{hair_length -> v...|  Cycling|hair_length|very short|\n",
      "|  Mikel|[Triathlon, Runni...|{hair_length -> v...|  Cycling|       hair|     blond|\n",
      "|Daniele|  [Singing, Dancing]|{hair_length -> l...|  Singing|hair_length|     large|\n",
      "|Daniele|  [Singing, Dancing]|{hair_length -> l...|  Singing|       hair|     brown|\n",
      "|Daniele|  [Singing, Dancing]|{hair_length -> l...|  Dancing|hair_length|     large|\n",
      "|Daniele|  [Singing, Dancing]|{hair_length -> l...|  Dancing|       hair|     brown|\n",
      "|   Anne|    [Running, Music]|{hair_length -> m...|  Running|hair_length|    middle|\n",
      "|   Anne|    [Running, Music]|{hair_length -> m...|  Running|       hair|       red|\n",
      "|   Anne|    [Running, Music]|{hair_length -> m...|    Music|hair_length|    middle|\n",
      "|   Anne|    [Running, Music]|{hair_length -> m...|    Music|       hair|       red|\n",
      "+-------+--------------------+--------------------+---------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.select('*', sql_f.explode('hobbies').alias('hobbie'))\\\n",
    "    .select('*', sql_f.explode('avatar').alias('clave', 'valor')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+-----+\n",
      "|hobbies                      |count|\n",
      "+-----------------------------+-----+\n",
      "|[Lifting, Running, Reading]  |1    |\n",
      "|[Triathlon, Running, Cycling]|1    |\n",
      "|[Singing, Dancing]           |1    |\n",
      "|[Running, Music]             |1    |\n",
      "+-----------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.groupBy('hobbies').count().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `lit(value)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---+\n",
      "|    name|age|  1|\n",
      "+--------+---+---+\n",
      "|   Alice|  1|  1|\n",
      "|     Bob|  4|  1|\n",
      "|    Juan| 10|  1|\n",
      "|    Pepe| 25|  1|\n",
      "|Panchito| 15|  1|\n",
      "+--------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('*', sql_f.lit(1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "print(\"Columna de unos: \")\n",
    "print(F.lit(1))\n",
    "\n",
    "print(\"\\nDataFrame con una nueva columna con unos:\")\n",
    "df.select(\"*\", F.lit(1)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `length(col) `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columna para obtener longitud del texto: \" )\n",
    "print(F.length(df.name))\n",
    "\n",
    "print(\"\\nDataFrame con la longitud de cada nombre\")\n",
    "df.select(F.length(df.name).alias('len')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaciones con pseudo-conjuntos\n",
    "Aunque no los hemos visto en teoría, podemos seguir haciendo operaciones entre pseudo-conjuntos al igual que con los RDDs\n",
    "\n",
    "Transformación | Descripción\n",
    "------------- | -------------\n",
    "*distinct()* | Devuelve el DataFrame sin elementos repetidos – ¡Cuidado! Requiere shuffle (enviar datos por red)\n",
    "*union(rdd)* | Devuelve la unión de los elementos en los dos DataFrame  (se mantienen los duplicados)\n",
    "*intersect(rdd)* | Devuelve la instersección de los elementos en los dos DataFrame (elimina los duplicados) – ¡Cuidado! Requiere shuffle (datos por red)\n",
    "*subtract(rdd)* | Devuelve los elementos presentes en el primer DataFrame y no en el segundo – ¡Cuidado! También requiere de shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame([\"agua\", \"vino\", \"cerveza\", \"agua\", \"agua\", \"vino\"], \"string\")\n",
    "df2 = spark.createDataFrame([\"cerveza\", \"cerveza\", \"agua\", \"agua\", \"vino\", \"coca-cola\", \"naranjada\"], \"string\")\n",
    "\n",
    "print(\"distinct: \" + str(df1.distinct().collect()))\n",
    "print(\"union: \" + str(df1.union(df2).collect()))\n",
    "print(\"intersect: \" + str(df1.intersect(df2).collect()))\n",
    "print(\"substract: \" + str(df1.subtract(df2).collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acciones sobre RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acciones básicas\n",
    "\n",
    "\n",
    "Acción | Descripción\n",
    "------------- | -------------\n",
    "*show(n=20, truncate=True)* | Imprime las primeras n filas del DataFrame. Truncate indica si se quiere truncar los strings demasiado largos\n",
    "*count()* | Devuelve el número de filas en el DataFrame\n",
    "*collect()* | Devuelve todas las filas del DataFrame como una lista de Rows **Cuidado: Debe de caber en memoria**\n",
    "*first()* | Devuelve la primera fila del DataFrame\n",
    "*take(n)* | Devuelve las primeras n filas del DataFrame como lista de Rows\n",
    "*toPandas()* | Devuelve el contenido del DataFrame como un pandas.DataFrame **Cuidado: Debe de caber en memoria**\n",
    "*columns* | Devuelve todos los nombres de columnas como una lista\n",
    "*describe(*\\**cols)* | Calcula estadísticas para las columnas numéricas  (count, mean, stddev, min, y max)\n",
    "*explain(extended=False)* | Imprime los planes físicos y lógicos para debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', age=1)]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(name='Alice', age=1)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'age']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----------------+\n",
      "|summary| name|              age|\n",
      "+-------+-----+-----------------+\n",
      "|  count|    5|                5|\n",
      "|   mean| null|             11.0|\n",
      "| stddev| null|9.513148795220223|\n",
      "|    min|Alice|                1|\n",
      "|    max| Pepe|               25|\n",
      "+-------+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DataFrame show(): \")\n",
    "df.show()\n",
    "\n",
    "print(\"Dos primeras filas con take(2): \" + str(df.take(2)))\n",
    "\n",
    "print(\"\\nPrimera fila con first(): \" + str(df.first()))\n",
    "\n",
    "print(\"\\nDataFrame completo con collect(): \" + str( df.collect() ))\n",
    "\n",
    "print(\"\\nNúmero de filas en el DataFrame con count(): \" + str( df.count() ) )\n",
    "\n",
    "print(\"\\nDataFrame como panda: \")\n",
    "print(df.toPandas())\n",
    "\n",
    "print(\"\\nColumnas en el DataFrame con columns: \"+ str( df.columns ))\n",
    "\n",
    "print(\"\\nPlanes físicos y lógicos con explain:\")\n",
    "df.filter(df.age > 10).select(df.age).explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaciones para realizar agregaciones\n",
    "\n",
    "\n",
    "\n",
    "Transformación | Descripción\n",
    "------------- | -------------\n",
    "*agg(*\\**exprs)* | Realiza agregaciones sobre el DataFrame completo sin agrupar (equivalente a df.groupBy.agg()) – Agregaciones disponibles: avg, max, min, sum, count. exprs puede ser un diccionario clave (nombre columna) – valor (función de agregación) o una lista de expresiones de agregación de Columna\n",
    "*groupBy(*\\**cols)* | Agrupa el DataFrame usando las columnas especificadas para poder aplicar agregaciones sobre los grupos – GroupedData\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `agg(*exprs)`\n",
    "Agrega columnas numéricas en todo el DataFrame, sin grupos\n",
    "\n",
    "Agregaciones disponibles: avg, max, min, sum, count\n",
    "\n",
    "\\**exprs* puede ser\n",
    "\n",
    "* Un diccionario clave (nombre columna) – valor (función de agregación)\n",
    "+ Una lista de expresiones de agregación de Columna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|avg(age)|max(age)|\n",
      "+--------+--------+\n",
      "|    11.0|      25|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(sql_f.mean('age'), sql_f.max('age')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': 'mean'}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'age': 'max', 'age': 'mean'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|avg(age)|\n",
      "+--------+\n",
      "|    11.0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Usando diccionario clave-valor: \")\n",
    "df.agg({\"age\": \"max\"}).show()\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "print(\"Usando expresión de agregación en la columna:\")\n",
    "df.agg(F.min(df.age)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `groupBy(*cols)`\n",
    "Agrupa filas del DataFrame en base a las columnas especificadas\n",
    "\n",
    "Se crea un DataFrame GroupedData, posteriormente se trata de aplicar agregaciones por grupos (ver siguiente sección)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    name|age|\n",
      "+--------+---+\n",
      "|   Alice|  1|\n",
      "|     Bob|  4|\n",
      "|    Juan| 10|\n",
      "|    Pepe| 25|\n",
      "|Panchito| 15|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+\n",
      "|    name|avg(age)|max(age)|\n",
      "+--------+--------+--------+\n",
      "|   Alice|     1.0|       1|\n",
      "|     Bob|     4.0|       4|\n",
      "|    Juan|    10.0|      10|\n",
      "|    Pepe|    25.0|      25|\n",
      "|Panchito|    15.0|      15|\n",
      "+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('name').agg(sql_f.mean('age'), sql_f.max('age')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Media usando groupBy().avg() - Se agrupa todo el DataFrame = usar agg() directamente\")\n",
    "print(df.groupBy().avg().collect())\n",
    "\n",
    "print(\"\\nAgrupamos por nombre y para cada nombre calculamos la edad media:\")\n",
    "print(sorted(df.groupBy('name').agg({'age': 'mean'}).collect()))\n",
    "print(sorted(df.groupBy(df.name).avg().collect()))\n",
    "\n",
    "print(\"\\nAgrupamos por nombre y edad y contamos cuántos filas hay en cada grupo:\")\n",
    "sorted(df.groupBy(['name', df.age]).count().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaciones sobre GroupedData (groupBy)\n",
    "\n",
    "\n",
    "Transformación | Descripción\n",
    "------------- | -------------\n",
    "*avg(*\\**cols) / mean(*\\**cols)* | Calcula la media de los valores para cada grupo en cada columna numérica\n",
    "*count()* | Cuenta el número de registros (filas) en cada grupo\n",
    "*max(**\\**cols)* | Calcula el máximo valor para cada grupo en cada columna numérica\n",
    "*min(*\\**cols)* | Calcula el mínimo valor para cada grupo en cada columna numérica\n",
    "*sum(*\\**cols)* | Calcula la suma de los valores para cada grupo en cada columna numérica\n",
    "*pivot(pivot_col, values)* | Pivota sobre una columna del DataFrame para realizar después la agregación especificada. Values especifica los valores que aparecerán en las columnas. Si no se especifica lo calcula Spark (menos eficiente)\n",
    "*agg(*\\**exprs)* | Realiza agregaciones sobre cada grupo del DataFrame. Agregaciones disponibles: avg, max, min, sum, count. exprs puede ser un diccionario clave (nombre columna) – valor (función de agregación) o una lista de expresiones de agregación de Columna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `avg(*cols) / mean(*cols), count(), max(*cols), min(*cols), sum(*cols)`\n",
    "Podemos aplicar directamente cualquiera de estos métodos sobre GroupedData para obtener el resultado de la función correspondiente por grupos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = sc.parallelize([ \\\n",
    "    Row(name='Alice', age=5, height=80), \\\n",
    "    Row(name='Alice', age=5, height=80), \\\n",
    "    Row(name='Alice', age=10, height=80), \\\n",
    "    Row(name='Bob', age=5, height=90), \\\n",
    "    Row(name='Bob', age=10, height=100)]).toDF()\n",
    "\n",
    "print(\"DataFrame ejemplo\")\n",
    "df2.show()\n",
    "\n",
    "print(\"Edad media global con avg: \" + str( df2.groupBy().avg('age').collect() ))\n",
    "print(\"Edad y altura medias globales con avg: \" + str( df2.groupBy().avg('age', 'height').collect() ))\n",
    "\n",
    "print(\"Edad media global con mean\" + str(df2.groupBy().mean('age').collect() ))\n",
    "print(\"Edad y altura medias globales con mean\" + str( df2.groupBy().mean('age', 'height').collect() ))\n",
    "\n",
    "\n",
    "print(\"Media de edad y altura agrupoda por nombre: \")\n",
    "df2.groupBy('name').avg('age', 'height').show()\n",
    "\n",
    "print(\"Conteo de personas por edad:\")\n",
    "df2.groupBy(df2.age).count().show()\n",
    "\n",
    "print(\"Edad máxima global\" + str( df2.groupBy().max('age').collect() ) )\n",
    "print(\"Edad y altura máximas globales\" + str( df2.groupBy().max('age', 'height').collect() ))\n",
    "\n",
    "\n",
    "print(\"Edad mínima global\" + str( df2.groupBy().min('age').collect() ))\n",
    "print(\"Edad y altura minima globales\" + str( df2.groupBy().min('age', 'height').collect() ))\n",
    "\n",
    "print(\"Suma global de edades\" + str( df2.groupBy().sum('age').collect() ))\n",
    "print(\"Suma global de edades y altura\" + str( df2.groupBy().sum('age', 'height').collect() ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Edad media agrupado por nombre con avg: \" )\n",
    "df2.groupBy('name').avg('age').show()\n",
    "\n",
    "print(\"Altura media agrupado por edad con avg: \" )\n",
    "df2.groupBy('age').avg('height').show()\n",
    "\n",
    "print(\"Conteo de personas por altura:\")\n",
    "df2.groupBy(df2.height).count().show()\n",
    "\n",
    "print(\"Edad máxima por nombre\" )\n",
    "df2.groupBy('name').max('age').show()\n",
    "\n",
    "print(\"Edad media por nombre y altura\" )\n",
    "df2.groupBy('name', 'height').avg('age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `agg(*exprs)`\n",
    "Agrega columnas numéricas por grupos\n",
    "\n",
    "Agregaciones disponibles: avg, max, min, sum, count\n",
    "\n",
    "\\**exprs* puede ser\n",
    "\n",
    "* Un diccionario clave (nombre columna) – valor (función de agregación)\n",
    "+ Una lista de expresiones de agregación de Columna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Conteo de personas con el mismo nombre:\")\n",
    "df2.agg({\"*\": \"count\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "print(\"Edad mínima para cada nombre:\")\n",
    "df2.groupBy(df2.name).agg(F.min(df2.age)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('Alice',1,6, 'Mate'), ('Bob',2,8, 'Mate'), ('Alice',3,9, 'Lengua'), ('Bob',4,7, 'Lengua')]\n",
    "df = spark.createDataFrame(data, ['name', 'age', 'grade', 'subject'])\n",
    "\n",
    "print(\"Media de edad y notas de todos los alumnos\")\n",
    "df.groupBy().avg().show()\n",
    "\n",
    "print(\"Media de edad y notas de los alumnos con el mismo nombre\")\n",
    "df.groupBy('name').avg('age', 'grade').show()\n",
    "\n",
    "\n",
    "print(\"Número de alumnos con el mismo nombre\")\n",
    "df1 = df.groupBy(df.name)\n",
    "df1.agg({\"*\": \"count\"}).show() \n",
    "df.groupBy(df.name).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `pivot(pivot_col, values)`\n",
    "\n",
    "Pivota sobre una columna del DataFrame para realizar después la agregación especificada\n",
    "\n",
    "*values* especifica los valores que aparecerán en las columnas\n",
    "* Si no se especifica lo calcula Spark (menos eficiente)\n",
    "* Similar a pivot table de pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(2021,'Alice', 67, 60, 'COMP4008'),\n",
    "        (2021,'Alice', 67, 25, 'COMP4103'),\n",
    "        (2021,'Bob', 34, 70, 'COMP4008'),\n",
    "        (2021,'Bob', 34, 95, 'COMP4103'),\n",
    "        (2022,'Mikel', 67, 60, 'COMP4008'),\n",
    "        (2022,'Mikel', 67, 25, 'COMP4103'),\n",
    "        (2022,'Isaac', 34, 70, 'COMP4008'),\n",
    "        (2022,'Isaac', 34, 95, 'COMP4103') ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data, ['year', 'name', 'age',\\\n",
    "                                  'grade', 'module_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+-----+-----------+\n",
      "|year| name|age|grade|module_name|\n",
      "+----+-----+---+-----+-----------+\n",
      "|2021|Alice| 67|   60|   COMP4008|\n",
      "|2021|Alice| 67|   25|   COMP4103|\n",
      "|2021|  Bob| 34|   70|   COMP4008|\n",
      "|2021|  Bob| 34|   95|   COMP4103|\n",
      "|2022|Mikel| 67|   60|   COMP4008|\n",
      "|2022|Mikel| 67|   25|   COMP4103|\n",
      "|2022|Isaac| 34|   70|   COMP4008|\n",
      "|2022|Isaac| 34|   95|   COMP4103|\n",
      "+----+-----+---+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+----------+\n",
      "|year|min(grade)|max(grade)|avg(grade)|\n",
      "+----+----------+----------+----------+\n",
      "|2021|        25|        95|      62.5|\n",
      "|2022|        25|        95|      62.5|\n",
      "+----+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('year').agg(sql_f.min('grade'), sql_f.max('grade'), sql_f.avg('grade')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+---+----+----+\n",
      "|year|module_name|min|maxi| avg|\n",
      "+----+-----------+---+----+----+\n",
      "|2021|   COMP4103| 25|  95|60.0|\n",
      "|2021|   COMP4008| 60|  70|65.0|\n",
      "|2022|   COMP4103| 25|  95|60.0|\n",
      "|2022|   COMP4008| 60|  70|65.0|\n",
      "+----+-----------+---+----+----+\n",
      "\n",
      "Wall time: 6.31 s\n"
     ]
    }
   ],
   "source": [
    "%time df.groupBy('year', 'module_name')\\\n",
    "            .agg(sql_f.min('grade').alias('min'), sql_f.max('grade').alias('maxi'), sql_f.avg('grade').alias('avg')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+-------------+------------+------------+-------------+------------+\n",
      "|year|COMP4008_min|COMP4008_maxi|COMP4008_avg|COMP4103_min|COMP4103_maxi|COMP4103_avg|\n",
      "+----+------------+-------------+------------+------------+-------------+------------+\n",
      "|2021|          60|           70|        65.0|          25|           95|        60.0|\n",
      "|2022|          60|           70|        65.0|          25|           95|        60.0|\n",
      "+----+------------+-------------+------------+------------+-------------+------------+\n",
      "\n",
      "Wall time: 6.26 s\n"
     ]
    }
   ],
   "source": [
    "%time df.groupBy('year').pivot('module_name', ['COMP4008', 'COMP4103'])\\\n",
    "    .agg(sql_f.min('grade').alias('min'), sql_f.max('grade').alias('maxi'), sql_f.avg('grade').alias('avg')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Notas medias por alumno y asignatura (modo eficiente especificando asignaturas): \")\n",
    "df.groupBy(\"name\").pivot(\"subject\", [\"Mate\", \"Lengua\"]).avg(\"grade\").show()\n",
    "\n",
    "print(\"Notas medias por alumno y asignatura (modo no eficiente, dejando a Spark que busque los valores sobre los que pivotar): \")\n",
    "df.groupBy(\"name\").pivot(\"subject\").avg(\"grade\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aspectos avanzados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones definidas por el usuario (User Defined Functions, UDF)\n",
    "\n",
    "Podemos crear una función para trabajar sobre las columnas de los DataFrames mediante funciones definidas por el usuario. Sin embargo, debemos limitarnos a lo estríctamente necesario al ser mucho menos eficientes que las específicas de Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('hola')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sql_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'length(hola)'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_f.length('hola')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+-----+-----------+\n",
      "|year| name|age|grade|module_name|\n",
      "+----+-----+---+-----+-----------+\n",
      "|2021|Alice| 67|   60|   COMP4008|\n",
      "|2021|Alice| 67|   25|   COMP4103|\n",
      "|2021|  Bob| 34|   70|   COMP4008|\n",
      "|2021|  Bob| 34|   95|   COMP4103|\n",
      "|2022|Mikel| 67|   60|   COMP4008|\n",
      "|2022|Mikel| 67|   25|   COMP4103|\n",
      "|2022|Isaac| 34|   70|   COMP4008|\n",
      "|2022|Isaac| 34|   95|   COMP4103|\n",
      "+----+-----+---+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|length(name)|\n",
      "+------------+\n",
      "|           5|\n",
      "|           5|\n",
      "|           3|\n",
      "|           3|\n",
      "|           5|\n",
      "|           5|\n",
      "|           5|\n",
      "|           5|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(sql_f.length('name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_len = sql_f.udf(lambda x: len(x), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|<lambda>(name)|\n",
      "+--------------+\n",
      "|             5|\n",
      "|             5|\n",
      "|             3|\n",
      "|             3|\n",
      "|             5|\n",
      "|             5|\n",
      "|             5|\n",
      "|             5|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(f_len('name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sql_f.udf('int')\n",
    "def f_len(s):\n",
    "    return len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|f_len(name)|\n",
      "+-----------+\n",
      "|          5|\n",
      "|          5|\n",
      "|          3|\n",
      "|          3|\n",
      "|          5|\n",
      "|          5|\n",
      "|          5|\n",
      "|          5|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(f_len('name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "print(\"Longitud con función definida por el usuario (UDF):\")\n",
    "slen = udf(lambda s: len(s), IntegerType())\n",
    "df.select(slen(df.name).alias('slen')).show()\n",
    "\n",
    "print(\"Longitud con versión de SparkSQL, mucho más eficiente:\")\n",
    "from pyspark.sql.functions import length\n",
    "df.select(length(df.name).alias('length')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL en Spark\n",
    "Podemos trabajar directamente con SQL en Spark si registramos el DataFrame como tabla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+-----+-----------+\n",
      "|year| name|age|grade|module_name|\n",
      "+----+-----+---+-----+-----------+\n",
      "|2021|Alice| 67|   60|   COMP4008|\n",
      "|2021|Alice| 67|   25|   COMP4103|\n",
      "|2021|  Bob| 34|   70|   COMP4008|\n",
      "|2021|  Bob| 34|   95|   COMP4103|\n",
      "|2022|Mikel| 67|   60|   COMP4008|\n",
      "|2022|Mikel| 67|   25|   COMP4103|\n",
      "|2022|Isaac| 34|   70|   COMP4008|\n",
      "|2022|Isaac| 34|   95|   COMP4103|\n",
      "+----+-----+---+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+-----+-----------+\n",
      "|year| name|age|grade|module_name|\n",
      "+----+-----+---+-----+-----------+\n",
      "|2021|Alice| 67|   25|   COMP4103|\n",
      "|2021|  Bob| 34|   95|   COMP4103|\n",
      "|2022|Mikel| 67|   25|   COMP4103|\n",
      "|2022|Isaac| 34|   95|   COMP4103|\n",
      "+----+-----+---+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM people WHERE age > 2 AND module_name != 'COMP4008'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JOINs con SparkSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaciones Join tipo SQL\n",
    "\n",
    "Transformación | Descripción\n",
    "------------- | -------------\n",
    "*join(other, on, how)*  | Joins entre dos DataFrames\n",
    "\n",
    "* `other`: segunda tabla para el join\n",
    "* `on`: nombre de la columna por la que se realiza la unión. Puede ser una lista o una expresión join (Column)\n",
    "* `how`: tipo de unión entre inner, `outer, left_outer, right_outer, left_semi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [Row(name=u'Alice', age=1), Row(name=u'Bob', age=2)]\n",
    "data2 = [Row(name=u'Chris', height=80), Row(name=u'Bob', height=85)]\n",
    "df = spark.createDataFrame(data, ['age', 'name'])\n",
    "df2 = spark.createDataFrame(data2, ['height', 'name'])\n",
    "print(\"df:\")\n",
    "df.show()\n",
    "print(\"df2:\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `inner join`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Join por name\")\n",
    "df.join(df2, 'name').show()\n",
    "print(\"Join por name + select df.name y height\")\n",
    "df.join(df2, 'name').select(df.name, df2.height).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `fullOuterJoin`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Full outer join por name\")\n",
    "df.join(df2, 'name', 'outer').show()\n",
    "print(\"Full outer join por name + select df.name y height\")\n",
    "df.join(df2, 'name', 'outer').select('name', 'height').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `leftOuterJoin`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Left outer join por name\")\n",
    "df.join(df2, 'name', 'left_outer').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `rightOuterJoin`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Left outer join por name\")\n",
    "df.join(df2, 'name', 'right_outer').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Más ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('Alice',1,6, 'Mate'), \\\n",
    "        ('Bob',2,8, 'Mate'), \\\n",
    "        ('Alice',3,9, 'Lengua'), \\\n",
    "        ('Bob',4,7, 'Lengua')]\n",
    "df = spark.createDataFrame(data, ['name', 'course', 'grade', 'subject'])\n",
    "df2 = sc.parallelize([ \\\n",
    "    Row(name='Alice', age=5, height=80), \\\n",
    "    Row(name='Alice', age=5, height=80), \\\n",
    "    Row(name='Alice', age=10, height=80), \\\n",
    "    Row(name='Bob', age=5, height=90), \\\n",
    "    Row(name='Bob', age=10, height=100), \\\n",
    "    Row(name='Juan', age=5, height=90), \\\n",
    "    Row(name='Jaimito', age=6, height=120)]).toDF()\n",
    "\n",
    "print(\"df:\")\n",
    "df.show()\n",
    "print(\"df2:\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Inner join usando name\")\n",
    "df.join(df2, 'name').show()\n",
    "\n",
    "print(\"Outer join usando name + select name y height\")\n",
    "df.join(df2, 'name', 'outer').select('name', 'height').show()\n",
    "\n",
    "print(\"Outer join usando df.name == df2.name + select df.name y height\")\n",
    "df.join(df2, df.name == df2.name, 'outer').select(df.name, df2.height).show()\n",
    "\n",
    "print(\"Outer join usando condición con name y age = grade (a pesar de no tener sentido) + select name y age\")\n",
    "cond = [df.name == df2.name, df2.age == df.grade]\n",
    "df.join(df2, cond, 'outer').select(df.name, df2.age).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caché de DataFrames\n",
    "Si se va a reusar un DataFrame es conveniente cachearlo para que no se recalcule cada vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quijoteDF = spark.read.text(\"./datos/pg2000.txt\")\n",
    "palabrasQuijoteDF = quijoteDF.select(explode(split('value', ' ')).alias('word')).cache()\n",
    "print(\"Cabeza aparece \" + str( palabrasQuijoteDF.where(palabrasQuijoteDF.word.like(\"%cabeza%\")).count() ) + \" veces\")\n",
    "print(\"Lanza aparece \" + str( palabrasQuijoteDF.where(palabrasQuijoteDF.word.like(\"%lanza%\")).count() ) + \" veces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura y escritura de ficheros\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura\n",
    "Leer ficheros de texto, JSON o CVS es muy sencillo.\n",
    "\n",
    "Hay dos formas de hacerlo:\n",
    "* Usando `spark.read.tipo_fichero(fichero)`\n",
    "* Usando `spark.read.format(tipo_fichero).options(opciones).load(fichero)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText = spark.read.text(\"datos/pg2000.txt\")\n",
    "dfJSON = spark.read.json(\"datos/json.json\") # infiere el esquema\n",
    "dfCSV = spark.read.csv(\"datos/personas.csv\", inferSchema=True, header=True)\n",
    "\n",
    "print(\"Elementos en DataFrame a partir de datos/pg2000.txt: \" + str(dfText.count()) + \"\\nEsquema: \")\n",
    "print dfText.printSchema()\n",
    "print(\"Elementos en DataFrame a partir de datos/json.json: \" + str(dfJSON.count()) + \"\\nEsquema: \")\n",
    "print dfJSON.printSchema()\n",
    "print(\"Elementos en DataFrame a partir de datos/personas.csv: \" + str(dfCSV.count()) + \"\\nEsquema: \")\n",
    "print dfCSV.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText = spark.read.format(\"text\").load(\"datos/pg2000.txt\")\n",
    "dfJSON = spark.read.format(\"json\").load(\"datos/json.json\") # infiere el esquema\n",
    "dfCSV = spark.read.format(\"csv\").options(inferSchema=True, header=True).load(\"datos/personas.csv\")\n",
    "\n",
    "print(\"Elementos en DataFrame a partir de datos/pg2000.txt: \" + str(dfText.count()) + \"\\nEsquema: \")\n",
    "print(dfText.printSchema())\n",
    "print(\"Elementos en DataFrame a partir de datos/json.json: \" + str(dfJSON.count()) + \"\\nEsquema: \")\n",
    "print(dfJSON.printSchema())\n",
    "print(\"Elementos en DataFrame a partir de datos/personas.csv: \" + str(dfCSV.count()) + \"\\nEsquema: \")\n",
    "print(dfCSV.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escritura\n",
    "Escribir ficheros de texto, JSON o CVS es igual de fácil.\n",
    "\n",
    "**Nota: El fichero de salida se toma como directorio**\n",
    "\n",
    "Hay dos formas de hacerlo:\n",
    "* Usando `DataFrame.write.tipo_fichero(fichero)`\n",
    "* Usando `DataFrame.write.format(tipo_fichero).options(opciones).load(fichero)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText.write.text(\"datos/salidaTXT1.txt\")\n",
    "dfJSON.write.json(\"datos/salidaJSON1.json\")\n",
    "dfCSV.write.csv(\"datos/salidaCSV1.csv\", header=True)\n",
    "\n",
    "print(\"Ver datos escritos en datos/salidaTXT1.txt\")\n",
    "print(\"Ver datos escritos en datos/salidaJSON1.json\")\n",
    "print(\"Ver datos escritos en datos/salidaCSV1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText.write.format('text').save(\"datos/salidaTXT2.txt\")\n",
    "dfJSON.write.format('json').save(\"datos/salidaJSON2.json\")\n",
    "dfCSV.write.format('csv').save(\"datos/salidaCSV2.csv\", header=True)\n",
    "\n",
    "print(\"Ver datos escritos en datos/salidaTXT2.txt\")\n",
    "print(\"Ver datos escritos en datos/salidaJSON2.json\")\n",
    "print(\"Ver datos escritos en datos/salidaCSV2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Más opciones en: <http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Un ejemplo completo: WordCount simple con DataFrames\n",
    "\n",
    "*value* es el nombre de la columna por defecto al leer texto\n",
    "\n",
    "1. Aplicamos un select con explode que equivale a un flatMap\n",
    "1. Dentro del select dividimos la línea por espacios con split\n",
    " * A la columna resultante la denominamos Word con alias\n",
    " * Filtramos palabras vacías\n",
    "2. Agrupamos por palabra y contamos cuántas veces se repite cada una\n",
    "3. Ordenamos de manera descendente\n",
    "4. Mostramos el DataFrame obtenido\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "dfText = spark.read.format(\"text\").load(\"data/pg2000.txt\")\n",
    "dfText.select(explode(split(dfText[\"value\"], \" \")).alias(\"word\")) \\\n",
    "    .filter(\"word != ''\") \\\n",
    "    .groupBy(\"word\").count() \\\n",
    "    .sort(desc(\"count\")) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Por pasos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText = spark.read.format(\"text\").load(\"data/pg2000.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfText.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "| que|19429|\n",
      "|  de|17988|\n",
      "|   y|15894|\n",
      "|  la|10200|\n",
      "|   a| 9575|\n",
      "|    | 9504|\n",
      "|  el| 7957|\n",
      "|  en| 7898|\n",
      "|  no| 5611|\n",
      "|  se| 4690|\n",
      "| los| 4680|\n",
      "| con| 4047|\n",
      "| por| 3758|\n",
      "| las| 3423|\n",
      "|  lo| 3387|\n",
      "|  le| 3382|\n",
      "|  su| 3319|\n",
      "| don| 2533|\n",
      "| del| 2464|\n",
      "|  me| 2344|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfText.select(sql_f.explode(sql_f.split('value', ' ')).alias('word'))\\\n",
    "        .groupBy('word').count()\\\n",
    "        .sort(sql_f.desc('count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vemos lo que hay en el DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText.select(\"*\").show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dividimos las líneas en palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText.select(split(\"value\", \" \")) \\\n",
    "    .show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usamos explode para crear una Row con cada elemento del array en la columna actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText.select( \\\n",
    "      explode(split(\"value\", \" \")) \\\n",
    "    ).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renombramos la nueva columna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText.select( \\\n",
    "        explode(split(\"value\", \" \")) \\\n",
    "        .alias(\"word\") \\\n",
    "    ).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtramos palabras vacías\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText.select( \\\n",
    "           explode(split(\"value\", \" \")) \\\n",
    "          .alias(\"word\")) \\\n",
    "      .filter(\"word != ''\") \\\n",
    "      .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agrupamos por palabra y contamos el número de apariciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText.select( \\\n",
    "              explode(split(\"value\", \" \")) \\\n",
    "              .alias(\"word\")) \\\n",
    "        .filter(\"word != ''\") \\\n",
    "        .groupBy(\"word\").count() \\\n",
    "        .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordenamos de manera descendente por la cuenta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText.select( \\\n",
    "              explode(split(\"value\", \" \")) \\\n",
    "              .alias(\"word\")) \\\n",
    "        .filter(\"word != ''\") \\\n",
    "        .groupBy(\"word\").count() \\\n",
    "        .sort(desc(\"count\")) \\\n",
    "        .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
